{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats as ss\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# закомментить эти строки перед тем, как выгружать в html\n",
    "# plt.style.use('ggplot')\n",
    "# get_ipython().run_line_magic('matplotlib', 'inline')\n",
    "# from matplotlib_inline.backend_inline import set_matplotlib_formats\n",
    "# set_matplotlib_formats('svg')\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from fitter import Fitter, get_common_distributions, get_distributions\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import random\n",
    "import math\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage, fcluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_bug_st(df, max_score):\n",
    "\n",
    "    proxy = df.groupby('session_id').sum()\n",
    "    index_ch = proxy[proxy['max_score'] <= max_score].index\n",
    "    \n",
    "    df = df[df['session_id'].isin(index_ch)]\n",
    "    \n",
    "    return df\n",
    "\n",
    "def plot_scores(df):\n",
    "    all_scores = np.array(df[['session_id', 'score']].groupby('session_id').sum())\n",
    "    plt.xlabel('Число решенных задач')\n",
    "    plt.ylabel('Количество студентов с таким скором')\n",
    "    plt.hist(all_scores)\n",
    "    plt.show()\n",
    "    \n",
    "def plot_students(df):\n",
    "    all_students = np.array(df[['session_id', 'school_id']].groupby('school_id').count())\n",
    "    plt.xlabel('Число студентов в школе')\n",
    "    plt.ylabel('Количество школ с таким числом студентов')\n",
    "    plt.hist(all_students)\n",
    "    plt.show()\n",
    "    \n",
    "    plt.xlabel('Число студентов в школе')\n",
    "    plt.ylabel('Количество школ с таким числом студентов')\n",
    "    plt.hist(all_students[all_students < 200])    \n",
    "    plt.show()\n",
    "    \n",
    "def parewise_stat(n):\n",
    "    return n * (n - 1) / 2\n",
    "\n",
    "def metric_sim_wrong_ans_weighted_other(df, tasks):\n",
    "    \n",
    "    '''\n",
    "    tasks - список номеров заданий\n",
    "    '''\n",
    "    \n",
    "    X = df.copy()\n",
    "    X = X[X['verdict'] == 'wrong']\n",
    "    \n",
    "    ans_dict = dict(zip(list(X['school_id'].unique()), np.zeros(len(list(X['school_id'].unique())))))\n",
    "    \n",
    "    for sch in ans_dict.keys():\n",
    "        Y = X[X['school_id'] == sch]\n",
    "        for i in range(len(tasks)):\n",
    "            one_more_dict = Counter(Y[Y['task_no'] == tasks[i]]['users_answer'])\n",
    "            stat = np.array(list(one_more_dict.values()))\n",
    "            ans_dict[sch] += np.sum(parewise_stat(stat[stat > 1]))\n",
    "            \n",
    "                    \n",
    "    return np.array(list(ans_dict.values())), list(ans_dict.keys())\n",
    "\n",
    "def plot_scatter_stud_vs_metric(studs, res):\n",
    "    plt.scatter(studs, res)\n",
    "    plt.title('Распределение метрики на исходных данных')\n",
    "    plt.xlabel('Количество школьников в школе')\n",
    "    plt.ylabel('Значение метрики')\n",
    "    plt.show()\n",
    "    \n",
    "def plot_log_metric(log_all_t_weighted):\n",
    "    plt.hist(log_all_t_weighted)\n",
    "    plt.title('Распределение метрики на исходных данных')\n",
    "    plt.xlabel('Значение метрики')\n",
    "    plt.ylabel('Количество школ с такой метрикой')\n",
    "    plt.show()\n",
    "    \n",
    "def get_best_distribution(log_all_t_weighted):\n",
    "    data_train, data_test = train_test_split(log_all_t_weighted, test_size=0.3, random_state=42)\n",
    "    f = Fitter(data_train,\n",
    "           distributions= ['gamma',\n",
    "                          'beta',\n",
    "                          'burr',\n",
    "                          'gengamma'])\n",
    "    f.fit()\n",
    "    f.summary()\n",
    "    plt.show()\n",
    "    \n",
    "    best_dist = f.get_best(method = 'sumsquare_error')\n",
    "    dist_name = list(best_dist.keys())[0]\n",
    "    a, b, loc, scale = f.fitted_param['beta']\n",
    "    print('Q-Q Plot на тренировочной выборке')\n",
    "    res_train = ss.probplot(data_train, sparams=(a, b, loc, scale), dist='beta', plot=plt, fit=True)\n",
    "    plt.show()\n",
    "    print('Q-Q Plot на тестовой выборке')\n",
    "    res_test = ss.probplot(data_test, sparams=(a, b, loc, scale), dist='beta', plot=plt, fit=True)\n",
    "    plt.show()\n",
    "    \n",
    "    print(f'p-value того, что train и test принадлежат одному распределению: {ss.ks_2samp(data_train, data_test).pvalue}')\n",
    "    \n",
    "    params = ss.beta.fit(data_train)\n",
    "    return ss.beta(*params)\n",
    "\n",
    "def get_dict_of_wr_ans_probs(df):\n",
    "    X = df.copy()\n",
    "    X = X[X['verdict']=='wrong']\n",
    "    all_wrong_ans_dict = {}\n",
    "    for i in range(1, 9):\n",
    "        all_wrong_ans_dict[i] = dict(Counter(X[X['task_no'] == i]['users_answer']))\n",
    "        \n",
    "    for key in all_wrong_ans_dict.keys():\n",
    "        s_wrong_i = sum(all_wrong_ans_dict[key].values())\n",
    "        for key1 in all_wrong_ans_dict[key].keys():\n",
    "            all_wrong_ans_dict[key][key1] = all_wrong_ans_dict[key][key1] / s_wrong_i\n",
    "            \n",
    "    return all_wrong_ans_dict\n",
    "\n",
    "def get_wrong_ans(num_task, num_studs, distribution_dict):\n",
    "    '''\n",
    "    num_task - номер задания\n",
    "    num_studs - количество студентов\n",
    "    distribution_dict - словарь с распределением (см. предыдущую функцию)\n",
    "    '''\n",
    "    draw = np.random.choice(list(distribution_dict[num_task].keys()), num_studs, p=list(distribution_dict[num_task].values()), replace=True)\n",
    "    return draw\n",
    "\n",
    "def f(obs):\n",
    "    if obs == 0:\n",
    "        return 'wrong'\n",
    "    else:\n",
    "        return 'ok'\n",
    "    \n",
    "def get_task_id(num_studs, num_tasks):\n",
    "    ans = [1] * num_studs\n",
    "    for i in range(2, num_tasks + 1):\n",
    "        ans.extend([i] * num_studs)\n",
    "        \n",
    "    return ans\n",
    "\n",
    "def true_students(n, num_of_tasks, school_id, array_emp_dist_of_correct, dict_emp_dist_of_wrong_ans):\n",
    "    \n",
    "    '''\n",
    "    если генерировать честных студентов из вероятностей каждого задания\n",
    "    '''\n",
    "    \n",
    "    dim = n * num_of_tasks\n",
    "    results = ss.bernoulli(array_emp_dist_of_correct).rvs(size=(n, num_of_tasks))\n",
    "    score = results.flatten()\n",
    "    all_answers = get_wrong_ans(1, n, dict_emp_dist_of_wrong_ans)\n",
    "    for i in range(2, num_of_tasks+1):\n",
    "        all_answers = np.concatenate((all_answers, get_wrong_ans(i, n, dict_emp_dist_of_wrong_ans)))\n",
    "        \n",
    "    school_ids = dim * [school_id]\n",
    "    stud_id = np.array(range(0, dim)) % n + 1\n",
    "    \n",
    "    task_id = get_task_id(n, num_of_tasks)\n",
    "    \n",
    "    d = {\n",
    "        'session_id': stud_id, 'task_no': task_id, 'score': score, 'school_id': school_ids, 'users_answer': all_answers, \n",
    "    }\n",
    "            \n",
    "    d = pd.DataFrame(data=d)\n",
    "    d['verdict'] = d['score'].apply(f)\n",
    "    return d\n",
    "\n",
    "def cheat_students(max_studs_in_cluster, clusters, num_of_tasks, school_id, array_emp_dist_of_correct, dict_emp_dist_of_wrong_ans):\n",
    "    \n",
    "    '''\n",
    "    генерация нечестных студентов из вероятностей\n",
    "    '''\n",
    "    \n",
    "    X = true_students(1, num_of_tasks, school_id, array_emp_dist_of_correct, dict_emp_dist_of_wrong_ans)\n",
    "    X['session_id'] = X['session_id'] + 900\n",
    "    \n",
    "    for i in range(clusters):\n",
    "        stud_in_sample_cluster = random.randint(2, max_studs_in_cluster)\n",
    "        Y = true_students(1, num_of_tasks, school_id, array_emp_dist_of_correct, dict_emp_dist_of_wrong_ans)\n",
    "        Y = pd.concat([Y] * stud_in_sample_cluster)\n",
    "        Y['session_id'] = np.array(range(0, stud_in_sample_cluster * num_of_tasks)) // num_of_tasks + 1 + 1000 + 100 * i\n",
    "        X = pd.concat([X, Y])\n",
    "        \n",
    "    return X.reset_index(drop=True)\n",
    "\n",
    "def generate_cheat_school(share_of_true, n, num_of_tasks, school_id, array_emp_dist_of_correct, dict_emp_dist_of_wrong_ans, max_studs_in_cluster=6):\n",
    "    '''\n",
    "    share_of_true - доля честных учеников\n",
    "    n - среднее количество всех учеников \n",
    "    num_of_tasks - количество заданий\n",
    "    school_id - id школы\n",
    "    array_emp_dist_of_correct - распределение вероятностей правильного решения каждого задания\n",
    "    dict_emp_dist_of_wrong_ans - распределение неправильных ответов\n",
    "    max_studs_in_cluster - максимум школьников в кластере\n",
    "    '''\n",
    "    \n",
    "    n_true = int(n * share_of_true)\n",
    "    n_cheat = n - n_true\n",
    "    \n",
    "    test_df_true = true_students(n_true, num_of_tasks, school_id, array_emp_dist_of_correct, dict_emp_dist_of_wrong_ans)\n",
    "    \n",
    "    num_of_clusters = random.randint(1, n_cheat // 2)\n",
    "    max_cheat_studs_per_cluster = n_cheat // num_of_clusters + 1\n",
    "    \n",
    "    test_df_cheat = cheat_students(max_cheat_studs_per_cluster, num_of_clusters, num_of_tasks, school_id, array_emp_dist_of_correct, dict_emp_dist_of_wrong_ans)\n",
    "    \n",
    "    return pd.concat([test_df_true, test_df_cheat]).reset_index(drop=True)\n",
    "\n",
    "def get_true_power(n, sample):\n",
    "    left_border = n > np.mean(n) - np.std(n)\n",
    "    right_border = n < np.mean(n) + np.std(n)\n",
    "    x = sample * right_border * left_border\n",
    "    y = n * right_border * left_border\n",
    "    return y[y != 0], x[x != 0]\n",
    "\n",
    "def power_experiment(emp_p, all_wr_ans_probs, crytical_value, share_of_true_studs=0.7, number_of_students=50, tasks=8, rep=100):\n",
    "    stat_cheat = np.zeros(rep)\n",
    "    students_at_cheat = np.zeros(rep)\n",
    "\n",
    "    for i in tqdm(range(rep)):\n",
    "        test_cheat_df = generate_cheat_school(share_of_true_studs, number_of_students, tasks, 1, emp_p, all_wr_ans_probs)\n",
    "        up = metric_sim_wrong_ans_weighted_other(test_cheat_df, np.array(range(1, tasks+1)))[0]\n",
    "        down = (test_cheat_df.shape[0] / tasks) ** 2\n",
    "        stat_cheat[i] =  np.log(up / down)\n",
    "        students_at_cheat[i] = test_cheat_df.shape[0] / tasks\n",
    "        \n",
    "    ad_studs, ad_power = get_true_power(students_at_cheat, stat_cheat)\n",
    "\n",
    "    print(f'Доля обнаруженных нечестных школ: {len(ad_power[ad_power > crytical_value]) / len(ad_power)}')\n",
    "    plt.hist(ad_power)\n",
    "    plt.axvline(x=crytical_value, c='r')\n",
    "    plt.show()\n",
    "    \n",
    "def df_school_with_metric(school_ids, num_students, metric_raw):\n",
    "    metric_not_logged = metric_raw / num_students ** 2\n",
    "    d = {'schools': school_ids, 'metric_not_logged': metric_not_logged}\n",
    "    df_return = pd.DataFrame(data=d)\n",
    "    df_return = df_return[df_return['metric_not_logged'] > 0]\n",
    "    df_return['metric_logged'] = np.log(df_return['metric_not_logged'])\n",
    "    return df_return\n",
    "\n",
    "def aa_experiment(emp_p, all_wr_ans_probs, crytical_value, share_of_true_studs=1, tasks=8, rep=100):\n",
    "    \n",
    "    stat_cheat = np.zeros(rep)\n",
    "    students_at_cheat = np.zeros(rep)\n",
    "\n",
    "    for i in tqdm(range(rep)):\n",
    "        number_of_students = random.randint(10, 500)\n",
    "        test_df = true_students(number_of_students, tasks, 1, emp_p, all_wr_ans_probs)\n",
    "        up = metric_sim_wrong_ans_weighted_other(test_df, np.array(range(1, tasks+1)))[0]\n",
    "        down = (number_of_students) ** 2\n",
    "        stat_cheat[i] =  np.log(up / down)\n",
    "        \n",
    "    print(f'Доля отмеченных честных школ: {len(stat_cheat[stat_cheat > crytical_value]) / len(stat_cheat)}')\n",
    "    plt.hist(stat_cheat)\n",
    "    plt.axvline(x=crytical_value, c='r')\n",
    "    plt.show()\n",
    "    \n",
    "    return np.quantile(stat_cheat, q=0.95)\n",
    "def shuffle_school(df):\n",
    "    df_test = df[['session_id', 'school_id']]\n",
    "    df_test = df_test.drop_duplicates(subset=['session_id'])\n",
    "    df_test['school_id'] = np.random.permutation(df_test['school_id'].values)\n",
    "    X = df.copy()\n",
    "    X = X.merge(df_test, on='session_id')\n",
    "    X = X.drop(columns=['school_id_x'])\n",
    "    X = X.rename(columns={'school_id_y':'school_id'})\n",
    "    return X\n",
    "\n",
    "def new_aa_experiment(df, crytical_value, tasks=8, rep=1, alpha=0.95):\n",
    "    \n",
    "    res = 0\n",
    "    false_alarm = 0\n",
    "\n",
    "    stat_random = np.array([])\n",
    "    \n",
    "    for i in tqdm(range(rep)):\n",
    "    \n",
    "        test_df = shuffle_school(df)\n",
    "        up = metric_sim_wrong_ans_weighted_other(test_df, np.array(range(1, tasks+1)))[0]\n",
    "        down = (np.array(test_df[['session_id', 'school_id']].groupby('school_id').count()).flatten()) ** 2\n",
    "        dim = min(len(up), len(down))\n",
    "        stat_cheat = np.log(up[:dim] / down[:dim])\n",
    "        stat_cheat = stat_cheat[stat_cheat > -100]\n",
    "\n",
    "        stat_random = np.append(stat_random, stat_cheat)\n",
    "\n",
    "        res += np.quantile(stat_cheat, q=1-alpha)\n",
    "\n",
    "        false_alarm += len(stat_cheat[stat_cheat > crytical_value])\n",
    "\n",
    "    plt.hist(stat_cheat)\n",
    "    plt.title('Распределение метрики на случайных школах')\n",
    "    plt.axvline(x=crytical_value, color='r')\n",
    "    plt.xlabel('Значение метрики')\n",
    "    plt.show()\n",
    "    \n",
    "    return res / rep, false_alarm, stat_random\n",
    "\n",
    "def make_cheat_school(test_df, schools, share_of_cheat):\n",
    "    dfs = test_df[test_df['school_id'] == np.random.choice(schools)]\n",
    "    students = dfs['session_id'].unique()\n",
    "    n_cheat = int(dfs.shape[0] * share_of_cheat)\n",
    "    \n",
    "    ans = dfs.copy()\n",
    "    \n",
    "    for i in range(n_cheat):\n",
    "        to_append = dfs[dfs['session_id'] == np.random.choice(students)]\n",
    "        ans = pd.concat([ans, to_append], ignore_index=True)\n",
    "        \n",
    "    return ans\n",
    "\n",
    "def new_power_experiment(df, crytical_value, share_of_cheat=0.3, tasks=8, rep=100): \n",
    "    schools = df['school_id'].unique()\n",
    "    test_df = shuffle_school(df)\n",
    "    \n",
    "    res_stats = np.zeros(rep)\n",
    "    for j in tqdm(range(rep)):\n",
    "        try:\n",
    "            sample = make_cheat_school(test_df, schools, share_of_cheat)\n",
    "            up = metric_sim_wrong_ans_weighted_other(sample, np.array(range(1, tasks+1)))[0]\n",
    "            down = (sample.shape[0] // 8) ** 2\n",
    "            res_stats[j] = np.log(up / down)\n",
    "        except:\n",
    "            res_stats[j] = -200\n",
    "            continue\n",
    "        \n",
    "        \n",
    "    plt.hist(res_stats[res_stats > -100])\n",
    "    plt.title('Распределение метрики на искусственных нечестных школах')\n",
    "    plt.axvline(x=crytical_value, color='r')\n",
    "    plt.xlabel('Значение метрики')\n",
    "    plt.show()\n",
    "    \n",
    "    power = len(res_stats[res_stats > crytical_value]) / len(res_stats)\n",
    "    print(f'Доля найденных нечестых школ (а-ка мощность): {power}')\n",
    "    # print(f'Подставим бутсрап 95 квантиль по перемешанным школам')\n",
    "\n",
    "    return res_stats\n",
    "def get_1_type_error_vs_power(rand, cheat, distrib):\n",
    "    quantiles = np.ones(5) - np.array([0.01, 0.02, 0.03, 0.04, 0.05])\n",
    "    ans = {\n",
    "        'заявленная ошибка первого рода': [], 'ошибка первого рода по распределению': [], 'мощность, используя квантили распределения': [], 'мощность, используя квантили случайных школ': []\n",
    "    }\n",
    "    for quantile in quantiles:\n",
    "        ans['заявленная ошибка первого рода'].append(1 - quantile)\n",
    "\n",
    "        cryt_val_dist = distrib.ppf(q=quantile)\n",
    "        f_type_error = len(rand[rand > cryt_val_dist]) / len(rand)\n",
    "        ans['ошибка первого рода по распределению'].append(round(f_type_error, 3))\n",
    "\n",
    "        power_dist = len(cheat[cheat > cryt_val_dist]) / len(cheat)\n",
    "        ans['мощность, используя квантили распределения'].append(round(power_dist, 3))\n",
    "\n",
    "        cryt_val_random = np.quantile(rand, q=quantile)\n",
    "        power_random = len(cheat[cheat > cryt_val_random]) / len(cheat)\n",
    "        ans['мощность, используя квантили случайных школ'].append(round(power_random, 3))\n",
    "\n",
    "    df = pd.DataFrame(ans)\n",
    "    return df\n",
    "def ultimate_pipeline(data, bootstrap_quantile, alpha = 0.01, rep_aa=10, rep_b=1000, share_of_cheat=0.3):\n",
    "    # data = pd.read_csv(path_data)\n",
    "    # schools = pd.read_csv(path_schools)\n",
    "    # data = pd.merge(data, schools, how='left', on='session_id')\n",
    "    \n",
    "    max_score = int(round((np.mean(data.groupby('session_id').sum()['max_score'])), 0))\n",
    "    number_of_schools = len(data['school_id'].unique())\n",
    "        \n",
    "    data = drop_bug_st(data, max_score)\n",
    "    \n",
    "    emp_p = np.array(data.groupby('task_no').sum()['score']) / (data.shape[0] / max_score)\n",
    "    all_wr_ans_probs = get_dict_of_wr_ans_probs(data)\n",
    "        \n",
    "    # print('==========================')\n",
    "    # print('EDA')\n",
    "    # scores distribution\n",
    "    # plot_scores(data)\n",
    "    # students distribution\n",
    "    # plot_students(data)\n",
    "    all_students = np.array(data[['session_id', 'school_id']].groupby('school_id').count())\n",
    "    # print(f'Количество школ, где людей меньше 10: {len(all_students[all_students < 10])}, или {round(len(all_students[all_students < 10]) / len(all_students) * 100, 2)}%')\n",
    "    print('==========================')\n",
    "    \n",
    "    # подсчет метрики для исходного датафрейма\n",
    "    print('Подсчет метрики для исходного датафрейма')\n",
    "    \n",
    "    res, schools_ans = metric_sim_wrong_ans_weighted_other(data, np.array(range(1, max_score + 1))) \n",
    "    studs = np.array(data[['session_id', 'school_id']].groupby('school_id').count()['session_id'])[:len(res)]\n",
    "    metric = np.log(res / studs ** 2)\n",
    "    \n",
    "    df_of_metrics = df_school_with_metric(schools_ans, studs, res)\n",
    "    \n",
    "    # скаттер-плот метрики от количества школьников\n",
    "    plot_scatter_stud_vs_metric(studs, metric)\n",
    "    \n",
    "    for_plot = res / studs ** 2\n",
    "    all_t_weighted = for_plot[for_plot > 0]\n",
    "    log_all_t_weighted = np.log(all_t_weighted[all_t_weighted > 0])\n",
    "    \n",
    "    plot_log_metric(log_all_t_weighted)\n",
    "    \n",
    "    best_distribution = get_best_distribution(log_all_t_weighted)\n",
    "    \n",
    "    crytical_value = best_distribution.ppf(q=1 - alpha)\n",
    "    \n",
    "    print(f'Уровень значимости: {alpha}, критическое значение по распределению: {round(crytical_value, 3)} (область правая)')\n",
    "           \n",
    "    print('==========================')\n",
    "    print(f'АА-тестирование с перемешиванием')\n",
    "    new_aa_95_quantile, false_alarm, all_random_schools_metric = new_aa_experiment(data, crytical_value, tasks=8, rep=rep_aa, alpha=alpha)\n",
    "    print(f'{alpha} квантиль для случайных школ: {round(new_aa_95_quantile, 3)}')\n",
    "    print(f'Ошибка первого рода при уровне значимости {alpha}: {false_alarm / (number_of_schools * rep_aa)}')\n",
    "\n",
    "    if bootstrap_quantile == True:\n",
    "        crytical_value = new_aa_95_quantile\n",
    "        \n",
    "    print('==========================')\n",
    "    print('Тестирование мощности с перемешиванием')\n",
    "    \n",
    "    all_cheat_schools_metric = new_power_experiment(data, crytical_value, rep=rep_b, share_of_cheat=share_of_cheat)\n",
    "\n",
    "    # табличка ошибка 1ого рода VS мощность\n",
    "    table_precision_vs_power = get_1_type_error_vs_power(all_random_schools_metric, all_cheat_schools_metric, best_distribution)\n",
    "    \n",
    "    df_of_metrics['pvalue'] = best_distribution.sf(df_of_metrics['metric_logged'])\n",
    "    return df_of_metrics[df_of_metrics['metric_logged'] > crytical_value].sort_values(by=['metric_logged'], ascending=False), table_precision_vs_power\n",
    "def rename_cols(df):\n",
    "\n",
    "    '''\n",
    "    функция переименовывает колонки в датафрейме\n",
    "    df - исходный датафрейм\n",
    "    ->pd.DataFrame\n",
    "    '''\n",
    "    old = df.columns[2:]\n",
    "    new = ['ans' + str(old[num]) for num in range(len(old))]\n",
    "    d = dict(zip(old, new))\n",
    "    return df.rename(columns=d)\n",
    "\n",
    "def subst_correct(df, tasks=8):\n",
    "\n",
    "    '''\n",
    "    функция чистит данные \n",
    "    df - исходный датафрейм\n",
    "    tasks - количество заданий \n",
    "\n",
    "    ->pd.DataFrame\n",
    "    '''\n",
    "\n",
    "    X = np.array(df.iloc[:, 2:2+tasks])\n",
    "    Y = np.array(df.iloc[:, 2+tasks:])\n",
    "\n",
    "    for i in range(len(X)):\n",
    "        for j in range(tasks):\n",
    "            if X[i, j] == 1:\n",
    "                Y[i, j] = str(0)\n",
    "\n",
    "    df.iloc[:, 2+tasks:] = Y\n",
    "\n",
    "    return df\n",
    "\n",
    "# subst_correct(data_test, tasks)\n",
    "\n",
    "def transform_data(data, school, tasks=8):\n",
    "    data_test = data[data['school_id'] == school]\n",
    "    data_test_wide_score = pd.pivot(data_test, index=['session_id','school_id'], columns = 'task_no',values = 'score').reset_index()\n",
    "    data_test_wide_ans = pd.pivot(data_test, index=['session_id','school_id'], columns = 'task_no',values = 'users_answer').reset_index()\n",
    "    data_test_wide_ans = rename_cols(data_test_wide_ans)\n",
    "    if tasks == 8:\n",
    "        data_test = pd.concat([data_test_wide_score, data_test_wide_ans.drop(['session_id', 'school_id'], axis=1)], axis = 1)\n",
    "    \n",
    "    else:\n",
    "        data_test_wide_ans = data_test_wide_ans.drop(['session_id', 'school_id'], axis=1)\n",
    "        data_test_wide_ans = data_test_wide_ans.drop(tasks, axis=1)\n",
    "        data_test = pd.concat([data_test_wide_score, data_test_wide_ans], axis = 1)\n",
    "    \n",
    "    return subst_correct(data_test, tasks)\n",
    "\n",
    "def build_model(data_test, num_tasks=8):\n",
    "    data_test_trans = data_test.iloc[:, 2 + num_tasks:]\n",
    "    enc = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "    data_test_trans = enc.fit_transform(data_test_trans)\n",
    "    # print(data_test_trans.shape)\n",
    "    link = linkage(data_test_trans, 'average', 'cosine')\n",
    "    # dn = dendrogram(link, color_threshold = -1, orientation = \"right\")               \n",
    "    # plt.show()\n",
    "    \n",
    "    dist = link[:, 2]\n",
    "    dist_rev = dist[::]\n",
    "    idxs = range(1, len(dist) + 1)\n",
    "    dim = data_test.shape[0] // 10 * 9\n",
    "    # plt.plot(idxs[-dim:], dist_rev[-dim:], marker='o')\n",
    "    # plt.title('Расстояние между объединяемыми кластерами')\n",
    "    # plt.xlabel('Шаг объединения')\n",
    "    # plt.ylabel('Расстояние')\n",
    "\n",
    "    return data_test_trans, link\n",
    "\n",
    "def parewise_stat(n):\n",
    "    return n * (n - 1) / 2\n",
    "\n",
    "def metric_sim_wrong_ans_weighted_other_cluster(df, max_score = 8):\n",
    "\n",
    "    tasks = np.array(range(1, max_score + 1))\n",
    "    \n",
    "    '''\n",
    "    tasks - список номеров заданий\n",
    "    '''\n",
    "    \n",
    "    X = df.copy()\n",
    "    X = X[X['verdict'] == 'wrong']\n",
    "    \n",
    "    ans_dict = dict(zip(list(X['cluster'].unique()), np.zeros(len(list(X['cluster'].unique())))))\n",
    "    \n",
    "    for sch in ans_dict.keys():\n",
    "        Y = X[X['cluster'] == sch]\n",
    "        for i in range(len(tasks)):\n",
    "            one_more_dict = Counter(Y[Y['task_no'] == tasks[i]]['users_answer'])\n",
    "            stat = np.array(list(one_more_dict.values()))\n",
    "            ans_dict[sch] += np.sum(parewise_stat(stat[stat > 1]))\n",
    "            \n",
    "    to_pd = {\n",
    "        'cluster': list(ans_dict.keys()), \n",
    "        'cluster_metric': list(ans_dict.values())\n",
    "    }      \n",
    "    \n",
    "    return pd.DataFrame(data=to_pd)\n",
    "def cluster_analysis(data, school_num, cth):\n",
    "\n",
    "    data_sample = data[data['school_id'] == school_num]\n",
    "    data_test = transform_data(data_sample, school_num)\n",
    "    data_test_trans, link = build_model(data_test)\n",
    "\n",
    "    # cth = float(input('Введите порог для кластеризации'))\n",
    "\n",
    "    # print(f'Выбранный порог для разбиения: {cth}')\n",
    "\n",
    "    # dn = dendrogram(link, color_threshold = cth, orientation = \"right\")\n",
    "\n",
    "    n_tasks = 8\n",
    "    data_test['cluster'] = fcluster(link, cth, criterion='distance')\n",
    "    data_test['total'] = data_test.iloc[:, 2:2 + n_tasks].sum(axis=1)\n",
    "    clusters = data_test.groupby('cluster').count().sort_values(by='total', ascending=False)\n",
    "\n",
    "    id_cluster = data_test[['session_id', 'cluster']]\n",
    "    data_sample = data_sample.merge(id_cluster, how='left', on='session_id')\n",
    "    metric_cluster = metric_sim_wrong_ans_weighted_other_cluster(data_sample)\n",
    "\n",
    "    return data_test, metric_cluster.sort_values(by='cluster_metric', ascending=False)\n",
    "def get_dict_of_wr_ans_probs(df):\n",
    "    X = df.copy()\n",
    "    X = X[X['verdict'].isin(['wrong', 'none'])]\n",
    "    all_wrong_ans_dict = {}\n",
    "    for i in range(1, 9):\n",
    "        all_wrong_ans_dict[i] = dict(Counter(X[X['task_no'] == i]['users_answer']))\n",
    "        \n",
    "    for key in all_wrong_ans_dict.keys():\n",
    "        s_wrong_i = sum(all_wrong_ans_dict[key].values())\n",
    "        for key1 in all_wrong_ans_dict[key].keys():\n",
    "            all_wrong_ans_dict[key][key1] = all_wrong_ans_dict[key][key1] / s_wrong_i\n",
    "\n",
    "    return all_wrong_ans_dict\n",
    "\n",
    "def get_dict_of_ok_ans_probs(data, task_num = 8):\n",
    "    df = data.copy()    \n",
    "    cor_ans_dict = df[df['verdict'] == 'ok'].groupby('task_no').count() / (df.shape[0] / task_num)\n",
    "    cor_ans_dict = cor_ans_dict.reset_index()\n",
    "    cor_ans_dict = dict(zip(list(cor_ans_dict['task_no'].values), list(cor_ans_dict['id'].values)))\n",
    "\n",
    "    return cor_ans_dict\n",
    "\n",
    "# LH для конкретного кластера\n",
    "\n",
    "def get_cluster_lh(data, ids, dict_ok, dict_wrong, tasks=8):\n",
    "    \n",
    "    LH = 0\n",
    "    for id in ids:\n",
    "        df_id = data.copy()\n",
    "        df_id = df_id[df_id['session_id'] == id]\n",
    "        for i in (range(1, tasks+1)):\n",
    "\n",
    "            verdict = df_id[df_id['task_no'] == i]['verdict'].values[0]\n",
    "            answer = df_id[df_id['task_no'] == i]['users_answer'].values[0]\n",
    "            # try:\n",
    "            if verdict == 'ok':\n",
    "                LH += np.log(dict_ok[i])\n",
    "\n",
    "            else:\n",
    "                LH += np.log(dict_wrong[i][answer] * (1 - dict_ok[i]))\n",
    "            # except:\n",
    "            #    LH += np.log(0.5)\n",
    "\n",
    "    return LH\n",
    "\n",
    "# LH для фиксированного кол-ва школьников \n",
    "\n",
    "def get_lh_bootstrap(n, cor_ans_dict, wr_ans_dict, tasks=8):\n",
    "    LH = 0\n",
    "    for i in range(tasks):\n",
    "        pvals = [cor_ans_dict[i + 1]]\n",
    "        pvals.extend(list(np.array(list(wr_ans_dict[i + 1].values())) * (1 - cor_ans_dict[i + 1])))\n",
    "        dist = ss.rv_discrete(values=(pvals, pvals))\n",
    "        LH += np.sum(np.log(dist.rvs(size=n)))\n",
    "\n",
    "    return LH\n",
    "\n",
    "def get_lh_distribution(n, cor_ans_dict, wr_ans_dict, tasks=8, rep=1000):\n",
    "    all_LH = np.zeros(rep)\n",
    "    for i in range(rep):\n",
    "        all_LH[i] = get_lh_bootstrap(n, cor_ans_dict, wr_ans_dict)\n",
    "\n",
    "    return all_LH\n",
    "\n",
    "def get_randomness_proba(obs, dist):\n",
    "    return (np.sum(dist < obs)) / len(dist)\n",
    "def convert_to_answer(dict_data):\n",
    "\n",
    "    ans = pd.DataFrame({'school_cluster_id': -1, 'session_id': [0], 'LH': 1, 'prob_LH': 2})\n",
    "\n",
    "    for school in dict_data.keys():\n",
    "        for cluster in dict_data[school].keys():\n",
    "            df = pd.DataFrame(dict_data[school][cluster])\n",
    "            ans = pd.concat([ans, df],  ignore_index=True)\n",
    "\n",
    "    ans = ans.iloc[1:, :]\n",
    "\n",
    "    return ans.sort_values(by=['prob_LH', 'school_cluster_id'])\n",
    "\n",
    "\n",
    "def plot_LH_dist(df):\n",
    "    X = df.copy()\n",
    "    plt.hist(X.drop_duplicates(subset=['school_cluster_id'])['prob_LH'])\n",
    "    plt.title('Распределение логправдоподобия по кластерам')\n",
    "    plt.xlabel('LH кластера')\n",
    "    plt.ylabel('Количество кластеров')\n",
    "    plt.show()\n",
    "def uber_ultimate_pipeline(data_path, schools_path, alpha = 0.01, rep_aa=10, rep_b=500, rep_lh=1000, cth=0.3, bootstrap_quantile=False, share_of_cheat=0.3):\n",
    "\n",
    "    data = pd.read_csv(data_path)\n",
    "    schools = pd.read_csv(schools_path)\n",
    "    data = pd.merge(data, schools, how='left', on='session_id')\n",
    "\n",
    "    # metric estimation and bad boys picking\n",
    "    X = data.copy()\n",
    "    res_pipeline, table_precision_vs_power = ultimate_pipeline(X, rep_aa=rep_aa, rep_b=rep_b, bootstrap_quantile=bootstrap_quantile, alpha=alpha, share_of_cheat=share_of_cheat)\n",
    "\n",
    "    susp_schools = list(res_pipeline['schools'].values)\n",
    "\n",
    "    # precompute probs\n",
    "\n",
    "    cor_ans_dict = get_dict_of_ok_ans_probs(data)\n",
    "    wr_ans_dict = get_dict_of_wr_ans_probs(data)\n",
    "\n",
    "    LH_probs_dict = {}\n",
    "    for i in tqdm(range(2, 6)):\n",
    "        LH_probs_dict[str(i)] = get_lh_distribution(i, cor_ans_dict, wr_ans_dict, rep=rep_lh)\n",
    "\n",
    "    # clusterization\n",
    "\n",
    "    Y = data.copy()\n",
    "    Y = Y[Y['school_id'].isin(susp_schools)]\n",
    "\n",
    "    # ans = get_clusterization_and_probs(Y, susp_schools, cor_ans_dict, wr_ans_dict, LH_probs_dict, cth)\n",
    "    ans_res = {}\n",
    "\n",
    "    for k in tqdm(range(len(susp_schools))):\n",
    "        Z = data.copy()\n",
    "\n",
    "        susp_school = susp_schools[k]\n",
    "\n",
    "        df_s, df_c = cluster_analysis(Z, susp_school, cth)\n",
    "        df_c = df_c[df_c['cluster_metric'] > 3]\n",
    "\n",
    "        susp_clusters = list(df_c['cluster'].values)\n",
    "\n",
    "        proxy_dict = {}\n",
    "\n",
    "        for j in (range(len(susp_clusters))):\n",
    "            W = data.copy()\n",
    "            sm_cluster = susp_clusters[j]\n",
    "            \n",
    "            sm_ids = list(df_s[df_s['cluster'] == sm_cluster]['session_id'].values)\n",
    "            \n",
    "            num_of_studs = len(sm_ids)\n",
    "\n",
    "            my_index = str(susp_school) + '_' + str(sm_cluster)\n",
    "\n",
    "            if num_of_studs <= 25:\n",
    "\n",
    "                try:\n",
    "                    sm_lh = get_cluster_lh(Z, sm_ids, cor_ans_dict, wr_ans_dict)\n",
    "                    sm_ls_probs = get_randomness_proba(sm_lh, LH_probs_dict[str(num_of_studs)])\n",
    "                    proxy_dict[sm_cluster] = {'school_cluster_id': my_index, 'session_id': sm_ids, 'LH':sm_lh, 'prob_LH':sm_ls_probs}\n",
    "                except:\n",
    "                    LH_probs_dict[str(num_of_studs)] = get_lh_distribution(num_of_studs, cor_ans_dict, wr_ans_dict, rep=rep_lh)\n",
    "                    sm_lh = get_cluster_lh(Z, sm_ids, cor_ans_dict, wr_ans_dict)\n",
    "                    sm_ls_probs = get_randomness_proba(sm_lh, LH_probs_dict[str(num_of_studs)])\n",
    "                    proxy_dict[sm_cluster] = {'school_cluster_id': my_index, 'session_id': sm_ids, 'LH':sm_lh, 'prob_LH':sm_ls_probs}\n",
    "\n",
    "        ans_res[susp_school] = proxy_dict\n",
    "\n",
    "    cluster_df_ans = convert_to_answer(ans_res) \n",
    "    cluster_df_ans = cluster_df_ans[cluster_df_ans['prob_LH'] < 1]\n",
    "    plot_LH_dist(cluster_df_ans)   \n",
    "\n",
    "    return res_pipeline, cluster_df_ans, table_precision_vs_power"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Олимпиада номер 2090"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'C:/Sirius/contests/'\n",
    "data_path = path + '2090/' + 'results.csv'\n",
    "schools_path = path + '2090/' + 'schools.csv'\n",
    "\n",
    "cheat_df, cluster_df, table_precision_vs_power = uber_ultimate_pipeline(data_path, schools_path, rep_aa=10, rep_b=1000, rep_lh=1000, cth=0.3, alpha=0.05, bootstrap_quantile=False, share_of_cheat=0.3)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
